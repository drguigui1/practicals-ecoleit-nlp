{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_9Vw5CGuYfI"
      },
      "source": [
        "# Recurrent models 101"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqEV3eNzvABm"
      },
      "source": [
        "In the evolving landscape of Natural Language Processing (NLP), Recurrent Neural Networks (RNNs) stand out for their unique ability to process sequential data, making them particularly suited for text analysis tasks. This practical exercise is designed to provide hands-on experience with RNNs, focusing on their application in classifying product reviews as either positive or negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GhY0dSAvFKf"
      },
      "source": [
        "Product reviews, ubiquitous across online platforms, serve as valuable data for extracting consumer sentiments and preferences. However, the unstructured nature of text data poses a challenge for traditional machine learning models. RNNs, with their sequential data processing capability, offer a robust solution to this challenge. By considering the temporal dynamics of language, RNNs can capture contextual nuances critical for accurately interpreting sentiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TdanDtcvMXa"
      },
      "source": [
        "This practical will guide you through the end-to-end process of leveraging RNNs for sentiment analysis, encompassing the following steps:\n",
        "\n",
        "- Reading the Dataset: We begin by importing a dataset comprising product reviews, each tagged with additional information such as review summary, verification status of the purchase, timestamp, adjusted log votes, and a binary indicator of the review's sentiment (positive or negative).\n",
        "- Exploratory Data Analysis (EDA): Before delving into modeling, we will conduct a preliminary analysis to understand the dataset's characteristics and distribution.\n",
        "- Dataset Splitting: The dataset will be divided into training and validation sets to evaluate the model's performance.\n",
        "- Text Processing and Transformation: We will preprocess the text data, converting it into a format suitable for RNN processing.\n",
        "- Data Batching and Iterator Creation: This step involves creating batches of data and iterators for efficiently feeding data into the model during training.\n",
        "- Utilizing Pre-trained GloVe Word Embeddings: To enhance the model's understanding of language semantics, we will use GloVe (Global Vectors for Word Representation), a pre-trained word embedding.\n",
        "- Setting Hyperparameters and Building the Network: We will configure the RNN's hyperparameters and architecture.\n",
        "- Training the Model: The network will be trained on the prepared dataset.\n",
        "- Validation: The trained model's performance will be evaluated on the validation dataset.\n",
        "- Improvement Ideas: We will explore strategies for further enhancing the model's accuracy.\n",
        "\n",
        "\n",
        "An important distinction to note is that while RNNs excel in processing textual data, incorporating additional features such as timestamps, log_votes, and verification status requires integrating RNNs with traditional neural network models. This practical will thus not only familiarize you with RNNs but also illustrate how to combine them with other neural network architectures to leverage both sequential and non-sequential data for comprehensive analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqOwi5FRwMVb"
      },
      "source": [
        "In this practical you need to use torchtext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuoouBzuwWRn"
      },
      "source": [
        "Import torchtext, GloVe vocabulary and get_tokenizer function from torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zd5J3z9uwSkY"
      },
      "outputs": [],
      "source": [
        "import torch, torchtext\n",
        "from torchtext.vocab import GloVe\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HaCcPReouUI1"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import torch, torchtext\n",
        "import pandas as pd\n",
        "from torch import nn, optim\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XynBVyDYwg_z"
      },
      "source": [
        "You can download the dataset using this link: https://drive.google.com/file/d/1k1XD0XPRSCC4tGGX1LUN-uzrmlQNJjs2/view?usp=sharing\n",
        "\n",
        "Then you can upload the dataset on your drive or directly in colab.\n",
        "\n",
        "Now, Let's read the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gtmp1eC1wKeg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('./AMAZON-REVIEW-DATA-CLASSIFICATION.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display some information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpEjoj-nxhyJ"
      },
      "source": [
        "### Data analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOPSpw37xfSP"
      },
      "source": [
        "Display some examples from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iO3Di4FRySnX"
      },
      "source": [
        "Display the number of samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blYxtd19xhLk"
      },
      "source": [
        "Display the label proportion (number of positive labels / negative labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVv7aT-RyD8k"
      },
      "source": [
        "Display the value count for verified"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K62ef7qhxtSE"
      },
      "source": [
        "Remove NaN values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "occ4XfPUyY_t"
      },
      "source": [
        "### Split the dataset into training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-Plaf8dygiO"
      },
      "source": [
        "### Text processing and Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDBXd5Rfy5HK"
      },
      "source": [
        "Use the get tokenizer function from torchtext to download `basic_english` tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxeBrZ3CzEGj"
      },
      "source": [
        "Use Counter() from collection, that we will use to compute the histogram of our tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9wvSgRqzLEH"
      },
      "source": [
        "Update the counter for each samples of your dataset\n",
        "\n",
        "`counter.update(tokenizer(\"...\"))`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "b2kXFPrzzKk3"
      },
      "outputs": [],
      "source": [
        "for line in train_text:\n",
        "    # FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhg4o6-lzVgE"
      },
      "source": [
        "Let's display the counter. You can see that we have the count for each token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-b9A9pJzRtH"
      },
      "outputs": [],
      "source": [
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPeCzBkCza9B"
      },
      "source": [
        "Create a vocabulary with words seen at least 5 (min_freq) times\n",
        "\n",
        "use torchtext vocab to create your vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EA1rQ0dOydBz"
      },
      "outputs": [],
      "source": [
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvLnU-xFoNMq"
      },
      "source": [
        "Add the unknow token and the pad token, using `insert_token` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a8j-oxm0L7O"
      },
      "outputs": [],
      "source": [
        "# Add the unknown token\n",
        "# and use it by default for unknown words\n",
        "unk_token = '<unk>'\n",
        "vocab.insert_token(unk_token, 0)\n",
        "vocab.set_default_index(0)\n",
        "\n",
        "# Add the pad token\n",
        "pad_token = '<pad>'\n",
        "vocab.insert_token(pad_token, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKEkG3looXz0"
      },
      "source": [
        "Here you have some examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6EWdzgIoVNE"
      },
      "outputs": [],
      "source": [
        "print(f\"'home' -> {vocab['home']}\")\n",
        "print(f\"'wash' -> {vocab['wash']}\")\n",
        "# unknown word (assume from test set)\n",
        "print(f\"'fhshbasdhb' -> {vocab['fhshbasdhb']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqC9s3A_oVXO"
      },
      "outputs": [],
      "source": [
        "# you can use the following mapper to tokenize the data\n",
        "text_transform_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7tr7cYAEe2j"
      },
      "outputs": [],
      "source": [
        "print(f\"Before transform:\\t{train_text[37]}\")\n",
        "print(f\"After transform:\\t{text_transform_pipeline(train_text[37])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e375wYg6EroP"
      },
      "source": [
        "Let's write a function that modifies and pads our text data as needed. In this function, we'll truncate the text sequence when it exceeds a specified length (in this case, max_len=50). If the text is shorter than max_len, we'll append 1s to the end of the sequence, representing the padding token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWtGIIKXEsTD"
      },
      "outputs": [],
      "source": [
        "def transformText(text_list, max_len):\n",
        "    # Transform the text\n",
        "    transformed_data = [text_transform_pipeline(text)[:max_len] for text in text_list]\n",
        "\n",
        "    # Pad zeros if the text is shoter than max_len\n",
        "    # FIXME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cw8gJ7j6cekz"
      },
      "outputs": [],
      "source": [
        "train_text[300]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6amnvBJchOa"
      },
      "outputs": [],
      "source": [
        "text = train_text[7:9]\n",
        "print(f\"Text: {text}\\n\")\n",
        "print(f\"Num sentences: {len(text)}\\n\")\n",
        "tt = transformText(text, max_len=50)\n",
        "print(f\"Transformed text: \\n{tt}\\n\")\n",
        "print(f\"Shape of transformed text: {tt.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anZmiuiMcxwP"
      },
      "source": [
        "Let's use the transformText() function and create the data loaders. Here, we use max_len=100 to consider the first 100 words in the text.\n",
        "\n",
        "Use TensorDataset to build the dataset from you tokens and labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbPzTjMlcmZu"
      },
      "outputs": [],
      "source": [
        "max_len = 100\n",
        "batch_size = 32\n",
        "\n",
        "# Pass transformed and padded data to dataset\n",
        "# Create data loaders\n",
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXGF7fRMduP2"
      },
      "source": [
        "### Build the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiYwDPX-c_V_"
      },
      "source": [
        "In this illustration, we'll leverage GloVe word vectors, specifically those from the '6B' dataset with dimensions of 300. This dataset encompasses 6 billion words and phrases, each represented by a vector containing 300 numerical values. The code below demonstrates how to access these word vectors and construct an embedding matrix. We'll establish a linkage between our vocabulary indices and the GloVe embeddings utilizing the get_vecs_by_tokens() function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ofx9VR68c_nM"
      },
      "outputs": [],
      "source": [
        "glove = GloVe(name=\"6B\", dim=300)\n",
        "embedding_matrix = # FIXME (use get vec by token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJA7VyqNdFh3"
      },
      "source": [
        "Hyper parameters of the model that you can tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rG7gNQ7EdBg8"
      },
      "outputs": [],
      "source": [
        "# Size of the state vectors\n",
        "hidden_size = 8\n",
        "\n",
        "# General NN training parameters\n",
        "learning_rate = 0.001\n",
        "epochs = 25\n",
        "\n",
        "# Embedding vector and vocabulary sizes\n",
        "embed_size = 300  # glove.6B.300d.txt\n",
        "vocab_size = len(vocab.get_itos())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzHO1xpNdRPE"
      },
      "source": [
        "Before proceeding, it's essential to ensure our data is properly formatted for the subsequent stages. Our model comprises the following layers:\n",
        "\n",
        "- Embedding Layer: Responsible for mapping words/tokens to word vectors.\n",
        "- RNN Layer: In this instance, we employ a simple RNN model, consisting of two stacked RNN layers. Further insights into the RNN architecture can be found here.\n",
        "- Linear Layer: Utilized for the final prediction of 'isPositive', this layer consists of a single neuron."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b9U-xlvdNgz"
      },
      "source": [
        "Embedding documentation: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqZgUeqkdO0b"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):\n",
        "        super().__init__()\n",
        "        # FIXME\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # FIXME\n",
        "\n",
        "# Initialize the weights\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "    if type(m) == nn.RNN:\n",
        "        for param in m._flat_weights_names:\n",
        "            if \"weight\" in param:\n",
        "                nn.init.xavier_uniform_(m._parameters[param])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G5h6IqjdZNe"
      },
      "source": [
        "Let's initialize this network. Then, we will need to make the embedding layer use our GloVe word vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4P-YG9OdffV"
      },
      "source": [
        "Use `model.embedding.weight.data.copy_` to set the pretrained embedding parameters and freeze the weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGGI1PGsdZm0"
      },
      "outputs": [],
      "source": [
        "# We set the embedding layer's parameters from GloVe\n",
        "# FIXME\n",
        "\n",
        "# We won't change/train the embedding layer, use requires_grad attributes\n",
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umK2FIEWdoVt"
      },
      "source": [
        "Build loss and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CW1UeSQKdpRV"
      },
      "outputs": [],
      "source": [
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EEzApEafbL_"
      },
      "source": [
        "Now build the train / validation loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eejt8Ugrfce2"
      },
      "outputs": [],
      "source": [
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh4diogNffIU"
      },
      "source": [
        "### Test our classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLEy1RTNfjOz"
      },
      "source": [
        "Display some prediction and display the confusion matrix / accuracy / precision / recall / f1score on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6OgMfihfgu-"
      },
      "outputs": [],
      "source": [
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZz7kCYtgcra"
      },
      "source": [
        "Improvement:\n",
        "\n",
        "- Change the batch size\n",
        "- Add more layers\n",
        "- Use GRU / LSTM layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQk1aqLYgiSv"
      },
      "outputs": [],
      "source": [
        "# FIXME"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
