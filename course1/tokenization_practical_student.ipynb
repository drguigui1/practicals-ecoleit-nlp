{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAOmSFyIqvP9"
      },
      "source": [
        "# Tokenizer 101\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St1zy4imqvP_"
      },
      "source": [
        "In today's practical session, we'll embark on an insightful journey into the world of Natural Language Processing (NLP), focusing specifically on the foundational role of tokenizers.\n",
        "\n",
        "In this activity you are going to:\n",
        "\n",
        "- Implement a simple word tokenization algorithm based on regular expressions.\n",
        "- Use one of the available tokenizers from NLTK and compare the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIa3dqudqvP_"
      },
      "source": [
        "## Task 1: Import data\n",
        "\n",
        "Import data from NLTK (see http://www.nltk.org/book_1ed/ch02.html):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIToTsXHqvQB"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('webtext')\n",
        "from nltk.corpus import webtext\n",
        "\n",
        "print(webtext.raw(\"pirates.txt\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wH8sqg1qvQD"
      },
      "source": [
        "Extract the first sentence as an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a39zZJLqvQE",
        "outputId": "a7019f47-6cbc-4ab1-d6d0-2b44b5381689"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PIRATES OF THE CARRIBEAN: DEAD MAN'S CHEST, by Ted Elliott & Terry Rossio\n"
          ]
        }
      ],
      "source": [
        "# Take the webtext.raw(\"pirates.txt\") as above\n",
        "# Split it by newline \\n, and extract the first sentence (i.e., at position 0)\n",
        "\n",
        "sentence = # FIXME\n",
        "print(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ2OS5bQqvQE"
      },
      "source": [
        "## Task 2: Implement a simple tokenization algorithm\n",
        "\n",
        "Split the sentence by whitespace:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJYIFZJ4qvQF",
        "outputId": "afe3577e-93c5-4c2d-f453-dd0f12275e06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['PIRATES', 'OF', 'THE', 'CARRIBEAN:', 'DEAD', \"MAN'S\", 'CHEST,', 'by', 'Ted', 'Elliott', '&', 'Terry', 'Rossio']\n"
          ]
        }
      ],
      "source": [
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYoZR5FoqvQF"
      },
      "source": [
        "Do you see any problems with this approach?\n",
        "\n",
        "Let's try pattern-matching (the code below applies a particular pattern, yours may be different):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc-dci8uqvQG",
        "outputId": "92467df0-6e83-48e0-ab43-9f8bd187c0e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['PIRATES', 'OF', 'THE', 'CARRIBEAN', 'DEAD', 'MAN', \"'\", 'S', 'CHEST', 'by', 'Ted', 'Elliott', '&', 'Terry', 'Rossio']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Define a pattern: e.g., r\"([-])+\" would split by \"-\" only; add other punctuation marks\n",
        "\n",
        "tokens = re.split(# FIXME)\n",
        "\n",
        "tokenized = [x for x in tokens if x != '' and x not in '- \\t\\n']\n",
        "print(tokenized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY8Zt1Bjs9nG"
      },
      "source": [
        "What about the following sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NjQQQnttJxN",
        "outputId": "65d172a6-3d74-46fb-b058-d65e50150cd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I', 'am', 'so', '\"happy\"', 'today']\n"
          ]
        }
      ],
      "source": [
        "sentence = \"I am so \\\"happy\\\" today\"\n",
        "\n",
        "tokens = re.split(# FIXME)\n",
        "\n",
        "tokenized = [x for x in tokens if x != '' and x not in '- \\t\\n']\n",
        "print(tokenized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRMLtvmUqvQH"
      },
      "source": [
        "Does this solve all the problems? Do you see any challenges?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sUjpONhqvQH"
      },
      "source": [
        "## Task 3: Apply NLTK tokenizer\n",
        "\n",
        "\n",
        "Now let's use a predefined tokenizer. It will help us a lot\n",
        "\n",
        "Use NLTK's tokenizer (see https://www.nltk.org/book/ch03.html):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IB14xJdDqvQH",
        "outputId": "94e2961e-3801-4a92-839f-5b97d9398ddb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I', 'am', 'so', '``', 'happy', \"''\", 'today']\n"
          ]
        }
      ],
      "source": [
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "tokens = # FIXME\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckJv9if7qvQI"
      },
      "source": [
        "Let's convert all words to lower case:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDt4CkjpqvQI",
        "outputId": "9f46cb77-a946-4094-f8b8-c242b5b3dca6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['i', 'am', 'so', '``', 'happy', \"''\", 'today']\n"
          ]
        }
      ],
      "source": [
        "words = # FIXME\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UgzOJMOqvQI"
      },
      "source": [
        "Finally, sort the words in alphabetical order (this will be our vocabulary):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jcwn1YOjqvQJ",
        "outputId": "ecaf4d92-06a3-4b44-e7ee-1d2f8daac006"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\"''\", '``', 'am', 'happy', 'i', 'so', 'today']\n"
          ]
        }
      ],
      "source": [
        "vocab = # FIXME\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now apply this to other sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzQ5ln4aqvQJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
