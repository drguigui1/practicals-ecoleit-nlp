{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtteQIE53ZBN"
      },
      "source": [
        "# Term-weighting techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnbZM25E3ZBP"
      },
      "source": [
        "In this practical you are going to:\n",
        "\n",
        "- Implement TF-IDF weighting\n",
        "- Apply these techniques to the collection of documents provided\n",
        "- Return the TF-IDF scores for the provided set of words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUzWiejz32OU"
      },
      "source": [
        "Prior to starting, engage in the exercise below:\n",
        "\n",
        "A variety of stemming algorithms are accessible through NLP toolkits, such as those included in NLTK. Typically, there's no need to create your own stemmer or lemmatizer from the ground up. Nonetheless, it's beneficial to familiarize yourself with the strengths and weaknesses of the toolkits that are already developed.\n",
        "\n",
        "For hands-on experience with various algorithms, consult (for further details, visit https://www.nltk.org/api/nltk.stem.html and https://www.nltk.org/howto/stem.html) and examine the differences in their outcomes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g0XCDhX3ZBQ"
      },
      "source": [
        "## Load the data\n",
        "\n",
        "There are three components to the provided `CISI` dataset (http://ir.dcs.gla.ac.uk/resources/test_collections/cisi/):\n",
        "- documents with their ids and content – there are $1460$ of those to be precise;\n",
        "- questions / queries with their ids and content – there are $112$ of those;\n",
        "- mapping between the queries and relevant documents.\n",
        "\n",
        "First, let's read in documents from the `CISI.ALL` file and store the result in `documents` data structure – set of tuples of document ids matched with contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJvy7sw83ZBS",
        "outputId": "48e93ae0-c623-478b-a002-75e3765ab8ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "1460 documents in total\n",
            "Document with id 1:\n",
            " 18 Editions of the Dewey Decimal Classifications Comaromi, J.P. The present study is a history of the DEWEY Decimal Classification.  The first edition of the DDC was published in 1876, the eighteenth edition in 1971, and future editions will continue to appear as needed.  In spite of the DDC's long and healthy life, however, its full story has never been told.  There have been biographies of Dewey that briefly describe his system, but this is the first attempt to provide a detailed history of the work that more than any other has spurred the growth of librarianship in this country and abroad. \n"
          ]
        }
      ],
      "source": [
        "#if using Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def read_documents():\n",
        "    f = open(\"/content/drive/My Drive/datasets_ecoleit/4NLP/data/CISI.ALL\")\n",
        "    merged = \"\"\n",
        "\n",
        "    for a_line in f.readlines():\n",
        "        if a_line.startswith(\".\"):\n",
        "            merged += \"\\n\" + a_line.strip()\n",
        "        else:\n",
        "            merged += \" \" + a_line.strip()\n",
        "\n",
        "    documents = {}\n",
        "\n",
        "    content = \"\"\n",
        "    doc_id = \"\"\n",
        "\n",
        "    for a_line in merged.split(\"\\n\"):\n",
        "        if a_line.startswith(\".I\"):\n",
        "            doc_id = a_line.split(\" \")[1].strip()\n",
        "        elif a_line.startswith(\".X\"):\n",
        "            documents[doc_id] = content\n",
        "            content = \"\"\n",
        "            doc_id = \"\"\n",
        "        else:\n",
        "            content += a_line.strip()[3:] + \" \"\n",
        "    f.close()\n",
        "    return documents\n",
        "\n",
        "documents = read_documents()\n",
        "print(f\"{len(documents)} documents in total\")\n",
        "print(\"Document with id 1:\")\n",
        "print(documents.get(\"1\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcjXk6GR3ZBT"
      },
      "source": [
        "Second, let's read in queries from the `CISI.QRY` file and store the result in `queries` data structure – set of tuples of query ids matched with contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tVR_1Sw3ZBT",
        "outputId": "88afea58-1800-4f56-d456-a9ced619eff9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "112 queries in total\n",
            "Query with id 1:\n",
            "What problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles? \n"
          ]
        }
      ],
      "source": [
        "def read_queries():\n",
        "    f = open(\"/content/drive/My Drive/datasets_ecoleit/4NLP/data/CISI.QRY\")\n",
        "    merged = \"\"\n",
        "\n",
        "    for a_line in f.readlines():\n",
        "        if a_line.startswith(\".\"):\n",
        "            merged += \"\\n\" + a_line.strip()\n",
        "        else:\n",
        "            merged += \" \" + a_line.strip()\n",
        "\n",
        "    queries = {}\n",
        "\n",
        "    content = \"\"\n",
        "    qry_id = \"\"\n",
        "\n",
        "    for a_line in merged.split(\"\\n\"):\n",
        "        if a_line.startswith(\".I\"):\n",
        "            if not content==\"\":\n",
        "                queries[qry_id] = content\n",
        "                content = \"\"\n",
        "                qry_id = \"\"\n",
        "            qry_id = a_line.split(\" \")[1].strip()\n",
        "        elif a_line.startswith(\".W\") or a_line.startswith(\".T\"):\n",
        "            content += a_line.strip()[3:] + \" \"\n",
        "    queries[qry_id] = content\n",
        "    f.close()\n",
        "    return queries\n",
        "\n",
        "queries = read_queries()\n",
        "print(f\"{len(queries)} queries in total\")\n",
        "print(\"Query with id 1:\")\n",
        "print(queries.get(\"1\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbGcHoL13ZBU"
      },
      "source": [
        "Finally, let's read in the mapping between the queries and the documents – we'll keep these in the `mappings` data structure – with tuples where each query index (key) corresponds to the list of one or more document indices (value):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeeqysej3ZBU",
        "outputId": "1c164b03-f4fb-490c-fc82-40585e21ea6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "76 mappings in total\n",
            "dict_keys(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '37', '39', '41', '42', '43', '44', '45', '46', '49', '50', '52', '54', '55', '56', '57', '58', '61', '62', '65', '66', '67', '69', '71', '76', '79', '81', '82', '84', '90', '92', '95', '96', '97', '98', '99', '100', '101', '102', '104', '109', '111'])\n",
            "Mapping for query with id 1:\n",
            "['28', '35', '38', '42', '43', '52', '65', '76', '86', '150', '189', '192', '193', '195', '215', '269', '291', '320', '429', '465', '466', '482', '483', '510', '524', '541', '576', '582', '589', '603', '650', '680', '711', '722', '726', '783', '813', '820', '868', '869', '894', '1162', '1164', '1195', '1196', '1281']\n"
          ]
        }
      ],
      "source": [
        "def read_mappings():\n",
        "    f = open(\"/content/drive/My Drive/datasets_ecoleit/4NLP/data/CISI.REL\")\n",
        "\n",
        "    mappings = {}\n",
        "\n",
        "    for a_line in f.readlines():\n",
        "        voc = a_line.strip().split()\n",
        "        key = voc[0].strip()\n",
        "        current_value = voc[1].strip()\n",
        "        value = []\n",
        "        if key in mappings.keys():\n",
        "            value = mappings.get(key)\n",
        "        value.append(current_value)\n",
        "        mappings[key] = value\n",
        "\n",
        "    f.close()\n",
        "    return mappings\n",
        "\n",
        "mappings = read_mappings()\n",
        "print(f\"{len(mappings)} mappings in total\")\n",
        "print(mappings.keys())\n",
        "print(\"Mapping for query with id 1:\")\n",
        "print(mappings.get(\"1\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW1Vx_6T3ZBV"
      },
      "source": [
        "## Preprocess the data\n",
        "\n",
        "Pratise application of the following steps:\n",
        "- tokenize the texts\n",
        "- put all to lowercase\n",
        "- remove stopwords\n",
        "- apply stemming\n",
        "\n",
        "Implement and apply these steps to a sample text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbdHX_-_3ZBV",
        "outputId": "27a7ae93-eda3-41f3-949d-b8ca197db065"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['cost', 'analys', 'sim', 'proc', 'evalu', 'larg', 'inform', 'system', 'bourn', 'c.p', 'ford', 'd.f', 'comput', 'program', 'writ', 'us', 'sim', 'several-year', 'op', 'inform', 'system', 'comput', 'estim', 'expect', 'op', 'cost', 'wel', 'amount', 'equip', 'personnel', 'requir', 'tim', 'period', 'program', 'us', 'analys', 'sev', 'larg', 'system', 'prov', 'us', 'research', 'tool', 'study', 'system', 'many', 'compon', 'interrel', 'op', 'equ', 'man', 'analys', 'would', 'extrem', 'cumbersom', 'tim', 'consum', 'perhap', 'ev', 'impract', 'pap', 'describ', 'program', 'show', 'exampl', 'result', 'sim', 'two', 'sev', 'suggest', 'design', 'spec', 'inform', 'system']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "\n",
        "def process(text):\n",
        "    stoplist = set(stopwords.words('english'))\n",
        "    st = LancasterStemmer()\n",
        "    word_list = [st.stem(word) for word in # FIXME\n",
        "                # A tokenized list of words, all converted to lowercase,\n",
        "                # If the word is not in the stoplist and not a punctuation mark (from string.punctuation)\n",
        "                ]\n",
        "    return word_list\n",
        "\n",
        "word_list = process(documents.get(\"27\"))\n",
        "print(word_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUwF_DFG3ZBV"
      },
      "source": [
        "## Step 3: Term weighing\n",
        "\n",
        "First calculate the term frequency in each document:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nyajb6d63ZBW",
        "outputId": "c89feb9d-250b-4d12-a092-6fe54462c01a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1460 documents in total\n",
            "Terms and frequencies for document with id 1:\n",
            "{'18': 1, 'edit': 4, 'dewey': 3, 'decim': 2, 'class': 2, 'comarom': 1, 'j.p.': 1, 'pres': 1, 'study': 1, 'hist': 2, 'first': 2, 'ddc': 2, 'publ': 1, '1876': 1, 'eighteen': 1, '1971': 1, 'fut': 1, 'continu': 1, 'appear': 1, 'nee': 1, 'spit': 1, \"'s\": 1, 'long': 1, 'healthy': 1, 'lif': 1, 'howev': 1, 'ful': 1, 'story': 1, 'nev': 1, 'told': 1, 'biograph': 1, 'brief': 1, 'describ': 1, 'system': 1, 'attempt': 1, 'provid': 1, 'detail': 1, 'work': 1, 'spur': 1, 'grow': 1, 'libr': 1, 'country': 1, 'abroad': 1}\n",
            "43 terms in this document\n",
            "\n",
            "112 queries in total\n",
            "Terms and frequencies for query with id 1:\n",
            "{'problem': 1, 'concern': 1, 'mak': 1, 'describ': 1, 'titl': 3, 'difficul': 1, 'involv': 1, 'autom': 1, 'retriev': 1, 'artic': 2, 'approxim': 1, 'us': 1, 'relev': 1, 'cont': 1}\n",
            "14 terms in this query\n"
          ]
        }
      ],
      "source": [
        "def get_terms(text):\n",
        "    terms = {}\n",
        "    st = LancasterStemmer()\n",
        "    stoplist = # FIXME: As above\n",
        "    word_list = # FIXME: As above\n",
        "    for word in word_list:\n",
        "        terms[word] = terms.get(word, 0) + 1\n",
        "    return terms\n",
        "\n",
        "doc_terms = {}\n",
        "qry_terms = {}\n",
        "for doc_id in documents.keys():\n",
        "    doc_terms[doc_id] = get_terms(# FIXME)\n",
        "for qry_id in queries.keys():\n",
        "    qry_terms[qry_id] = get_terms(# FIXME)\n",
        "\n",
        "\n",
        "print(f\"{len(doc_terms)} documents in total\") # Sanity check – this should be the same number as before\n",
        "d1_terms = doc_terms.get(\"1\")\n",
        "print(\"Terms and frequencies for document with id 1:\")\n",
        "print(d1_terms)\n",
        "print(f\"{len(d1_terms)} terms in this document\")\n",
        "print()\n",
        "print(f\"{len(qry_terms)} queries in total\") # Sanity check – this should be the same number as before\n",
        "q1_terms = qry_terms.get(\"1\")\n",
        "print(\"Terms and frequencies for query with id 1:\")\n",
        "print(q1_terms)\n",
        "print(f\"{len(q1_terms)} terms in this query\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrF0xeGE3ZBW"
      },
      "source": [
        "Second, collect shared vocabulary from all documents and queries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQQSPkgE3ZBW",
        "outputId": "198bf5d8-fa36-49d4-8f76-92cf784c5d14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7775 terms in the shared vocabulary\n",
            "First 10:\n",
            "[\"''\", \"'60\", \"'70\", \"'anyhow\", \"'apparent\", \"'basic\", \"'better\", \"'bibliograph\", \"'bibliometrics\", \"'building\"]\n"
          ]
        }
      ],
      "source": [
        "def collect_vocabulary():\n",
        "    all_terms = []\n",
        "    for doc_id in doc_terms.keys():\n",
        "        for term in doc_terms.get(doc_id).keys():\n",
        "            all_terms.append(term)\n",
        "    for qry_id in qry_terms.keys():\n",
        "        # FIXME\n",
        "        # Apply the same procedure to the query terms\n",
        "    return sorted(set(all_terms))\n",
        "\n",
        "all_terms = collect_vocabulary()\n",
        "print(f\"{len(all_terms)} terms in the shared vocabulary\") # This should be the same number as before\n",
        "print(\"First 10:\")\n",
        "print(all_terms[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtePDFtG3ZBW"
      },
      "source": [
        "Represent each document and query as vectors containing word counts in the shared space:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAGpJjFQ3ZBW",
        "outputId": "c541b0a5-bc0c-4c4e-e333-6c8813814bfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1460 document vectors\n",
            "7775 terms in this document\n",
            "112 query vectors\n",
            "7775 terms in this query\n"
          ]
        }
      ],
      "source": [
        "def vectorize(input_terms, shared_vocabulary):\n",
        "    output = {}\n",
        "    for item_id in input_terms.keys(): # e.g., a document in doc_terms\n",
        "        terms = input_terms.get(item_id)\n",
        "        output_vector = []\n",
        "        for word in shared_vocabulary:\n",
        "            if word in terms.keys():\n",
        "                # add the raw count of the word from the shared vocabulary in doc to the doc vector\n",
        "                output_vector.append(int(terms.get(word)))\n",
        "            else:\n",
        "                # if the word from the shared vocabulary is not in doc, add 0 to the doc vector in this position\n",
        "                output_vector.append(0)\n",
        "        output[item_id] = output_vector\n",
        "    return output\n",
        "\n",
        "doc_vectors = vectorize(\n",
        "    # Apply vectorize to the doc_terms and the shared vocabulary all_terms\n",
        ")\n",
        "qry_vectors = vectorize(\n",
        "    # Apply vectorize to the qry_terms and the shared vocabulary all_terms\n",
        ")\n",
        "\n",
        "print(f\"{len(doc_vectors)} document vectors\") # This should be the same number as before\n",
        "d1460_vector = doc_vectors.get(\"1460\")\n",
        "print(f\"{len(d1460_vector)} terms in this document\") # This should be the same number as before\n",
        "print(f\"{len(qry_vectors)} query vectors\") # This should be the same number as before\n",
        "q112_vector = qry_vectors.get(\"112\")\n",
        "print(f\"{len(q112_vector)} terms in this query\") # This should be the same number as before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oiCN3uO3ZBX",
        "outputId": "0b3175af-98ef-4180-9854-4dd0c62cc4e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7775 terms with idf scores\n",
            "Idf score for the word system:\n",
            "0.4287539560862571\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "def calculate_idfs(shared_vocabulary, d_terms):\n",
        "    doc_idfs = {}\n",
        "    for term in shared_vocabulary:\n",
        "        doc_count = 0 # the number of documents containing this term\n",
        "        for doc_id in d_terms.keys():\n",
        "            terms = d_terms.get(doc_id)\n",
        "            if term in terms.keys():\n",
        "                doc_count += 1\n",
        "        doc_idfs[term] = math.log(float(len(d_terms.keys()))/float(1 + doc_count), 10)\n",
        "    return doc_idfs\n",
        "\n",
        "doc_idfs = calculate_idfs(\n",
        "    # Apply calculate_idfs to the shared vocabulary all_terms and to doc_terms\n",
        ")\n",
        "print(f\"{len(doc_idfs)} terms with idf scores\") # This should be the same number as before\n",
        "print(\"Idf score for the word system:\")\n",
        "print(doc_idfs.get(\"system\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKNtZ52i3ZBX",
        "outputId": "05582104-01b2-4113-94c8-da52c363fa37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1460 document vectors\n",
            "Number of idf-scored words in a particular document:\n",
            "7775\n"
          ]
        }
      ],
      "source": [
        "def vectorize_idf(input_terms, input_idfs, shared_vocabulary):\n",
        "    output = {}\n",
        "    for item_id in input_terms.keys():\n",
        "        terms = # Collect terms from the document\n",
        "        output_vector = []\n",
        "        for term in shared_vocabulary:\n",
        "            if term in terms.keys():\n",
        "                output_vector.append(input_idfs.get(term)*float(terms.get(term)))\n",
        "            else:\n",
        "                output_vector.append(float(0))\n",
        "        output[item_id] = output_vector\n",
        "    return output\n",
        "\n",
        "doc_vectors = vectorize_idf(\n",
        "    # Apply to the relevant data structures\n",
        ")\n",
        "\n",
        "print(f\"{len(doc_vectors)} document vectors\") # This should be the same number as before\n",
        "print(\"Number of idf-scored words in a particular document:\")\n",
        "print(len(doc_vectors.get(\"1460\"))) # This should be the same number as before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuOqKDcL3ZBZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
